{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install wandb matplotlib rich\n",
        "!wandb login 45929b0a7ce4c8f23db3d4c3ebcfeb6f37019e62"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydv-pSqsO9vC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import wandb\n",
        "from rich import print\n",
        "import logging\n",
        "from rich.logging import RichHandler\n",
        "import matplotlib.pyplot as plt\n",
        "import inspect\n",
        "from contextlib import nullcontext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK7TdltxPihD"
      },
      "outputs": [],
      "source": [
        "FORMAT = \"%(message)s\"\n",
        "logging.basicConfig(\n",
        "    level=\"NOTSET\", format=FORMAT, datefmt=\"[%X]\", handlers=[RichHandler()]\n",
        ")\n",
        "\n",
        "log = logging.getLogger(\"rich\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "IgCHNaO9Pr13",
        "outputId": "90c7cf14-e269-4885-8431-7d32bfa91de1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Using device: xl</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">a:0</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32mUsing device: xl\u001b[0m\u001b[1;32ma:0\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Check for CUDA availability and setup\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"[green]CUDA is available with {torch.cuda.device_count()} GPUs[/green]\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"[blue]GPU {i}: {torch.cuda.get_device_name(i)}[/blue]\")\n",
        "else:\n",
        "    print(f\"[red]CUDA is not available, falling back to CPU[/red]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWZ1hYYxP9eY"
      },
      "outputs": [],
      "source": [
        "SEQ_LENGTH = 4096\n",
        "VOCAB_SIZE = 50304\n",
        "EMBEDDING_DIM = 1024\n",
        "NUM_HEADS = 16\n",
        "NUM_BLOCKS = 16\n",
        "BATCH_SIZE = 128\n",
        "NUM_EXPERTS = 64\n",
        "TOP_K_EXPERTS = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMlPCV4iP_fY"
      },
      "outputs": [],
      "source": [
        "def load_tokens(filename):\n",
        "    npt = np.load(filename)\n",
        "    npt = npt.astype(np.int32)\n",
        "    ptt = torch.tensor(npt, dtype=torch.long)\n",
        "    return ptt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "445XUq5mQAYs"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, B, T, split, data_root, process_rank=0, num_processes=1):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        assert split in {'train', 'val'}\n",
        "        data_root = data_root\n",
        "        shards = os.listdir(data_root)\n",
        "        shards = [s for s in shards if split in s]\n",
        "        shards = sorted(shards)\n",
        "        shards = [os.path.join(data_root, s) for s in shards]\n",
        "        self.shards = [s for i, s in enumerate(shards) if i % self.num_processes == self.process_rank]\n",
        "        if len(self.shards) == 0:\n",
        "            raise ValueError(f\"No shards for process {self.process_rank}\")\n",
        "        print(f\"[green]Process {self.process_rank} found {len(self.shards)} shards for split {split}[/green]\")\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.current_shard = 0\n",
        "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "        self.current_position = 0\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x = (buf[:-1]).view(B, T)\n",
        "        y = (buf[1:]).view(B, T)\n",
        "        self.current_position += B * T\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
        "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "            self.current_position = 0\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bj32er5QBY-"
      },
      "outputs": [],
      "source": [
        "class RotaryPositionEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_lenght):\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_lenght\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        pos = torch.arange(self.max_seq_len).float()\n",
        "        freqs = torch.einsum(\"i,j->ij\", pos, inv_freq)\n",
        "        self.cos = torch.cos(freqs)[None, None, :, :]\n",
        "        self.sin = torch.sin(freqs)[None, None, :, :]\n",
        "    def forward(self, x):\n",
        "        self.seq_len = x.size(2)\n",
        "        x1 = x[..., ::2]\n",
        "        x2 = x[..., 1::2]\n",
        "        cos = self.cos[:, :, :self.seq_len, :].to(x.device)\n",
        "        sin = self.sin[:, :, :self.seq_len, :].to(x.device)\n",
        "        x_rotated = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
        "        return x_rotated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wcO0Hl3QPlG"
      },
      "outputs": [],
      "source": [
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, input_dimension, hidden_dimension):\n",
        "        super().__init__()\n",
        "        # First linear layer outputs 2 * hidden_dimension for the gate and value\n",
        "        self.linear1 = nn.Linear(input_dimension, 2 * hidden_dimension, bias=True)\n",
        "        # Second linear layer takes hidden_dimension and outputs input_dimension\n",
        "        self.linear2 = nn.Linear(hidden_dimension, input_dimension, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        combined = self.linear1(x)\n",
        "        a, b = combined.chunk(2, dim=-1)\n",
        "        swish = b * torch.sigmoid(b)\n",
        "        output = self.linear2(swish * a)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2RAKi2MQQby"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, input_shape, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.g = nn.Parameter(torch.ones(input_shape))\n",
        "        self.b = nn.Parameter(torch.ones(input_shape))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
        "        output = x / rms\n",
        "        output = (output * self.g) + self.b\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6wtSW14QRKZ"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embed, n_head, max_seq_lenght, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.n_embd = n_embed\n",
        "        self.n_head = n_head\n",
        "        self.max_seq_lenght = max_seq_lenght\n",
        "        self.eps = eps\n",
        "        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd)\n",
        "        self.alpha = nn.Parameter(torch.ones(self.n_head))\n",
        "        self.c_proj = nn.Linear(self.n_embd, self.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        head_dim = self.n_embd // self.n_head\n",
        "        self.rope_q = RotaryPositionEmbedding(head_dim, self.max_seq_lenght)\n",
        "        self.rope_k = RotaryPositionEmbedding(head_dim, self.max_seq_lenght)\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q_norm = torch.norm(q, dim=-1, keepdim=True)\n",
        "        k_norm = torch.norm(k, dim=-1, keepdim=True)\n",
        "        q_hat = q / (q_norm + self.eps)\n",
        "        k_hat = k / (k_norm + self.eps)\n",
        "        factor = self.alpha * math.sqrt(C // self.n_head)\n",
        "        factor = factor.view(1, self.n_head, 1, 1)\n",
        "        q_scaled = q_hat * factor\n",
        "        k_scaled = k_hat * factor\n",
        "        q_rotated = self.rope_q(q_scaled)\n",
        "        k_rotated = self.rope_k(k_scaled)\n",
        "        y = F.scaled_dot_product_attention(q_rotated, k_rotated, v, is_causal=True, dropout_p=0.0)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alEHdgR1QSGW"
      },
      "outputs": [],
      "source": [
        "class Expert(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.expert = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            SwiGLU(4 * embed_dim, 4 * embed_dim),\n",
        "            nn.Linear(4 * embed_dim, embed_dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.expert(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE1CUkqVQTmI"
      },
      "outputs": [],
      "source": [
        "class Router(nn.Module):\n",
        "    def __init__(self, num_experts, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.embed_dim = embed_dim\n",
        "        self.linear = nn.Linear(self.embed_dim, self.num_experts)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear(x)\n",
        "        output = self.softmax(logits)\n",
        "        return output, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQWtYk_YQUeM"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_experts, max_seq_lenght):\n",
        "        super(Block, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_experts = num_experts\n",
        "        self.max_seq_lenght = max_seq_lenght\n",
        "\n",
        "        self.RMSNorm = RMSNorm(self.embed_dim)\n",
        "        self.MultiheadAttention = CausalSelfAttention(self.embed_dim, self.num_heads, self.max_seq_lenght)\n",
        "        self.router = Router(self.num_experts, self.embed_dim)\n",
        "        self.experts = nn.ModuleList([Expert(self.embed_dim) for _ in range(self.num_experts)])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.MultiheadAttention(self.RMSNorm(x))\n",
        "        routes, xj_logits = self.router(x)\n",
        "        top8_probs, top8_indices = torch.topk(routes, k=8, dim=2)\n",
        "        top8_probs = top8_probs / top8_probs.sum(dim=-1, keepdim=True)\n",
        "        expert_output = torch.zeros_like(x)\n",
        "        for k in range(8):\n",
        "            expert_idx = top8_indices[:, :, k]\n",
        "            prob = top8_probs[:, :, k]\n",
        "            for expert_id in range(self.num_experts):\n",
        "                mask = (expert_idx == expert_id)\n",
        "                if mask.sum() == 0:\n",
        "                    continue\n",
        "                x_selected = x[mask]\n",
        "                expert_out = self.experts[expert_id](x_selected)\n",
        "                prob_selected = prob[mask].unsqueeze(-1)\n",
        "                weighted_out = expert_out * prob_selected\n",
        "                expert_output[mask] += weighted_out\n",
        "\n",
        "        x = x + expert_output\n",
        "\n",
        "        return x, top8_indices, top8_probs, xj_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmTLvUsGQV1x"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, max_seq_lenght, num_heads, num_experts, num_blocks=16):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_lenght = max_seq_lenght\n",
        "        self.num_heads = num_heads\n",
        "        self.num_experts = num_experts\n",
        "        self.num_blocks = num_blocks\n",
        "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_dim)\n",
        "        self.blocks = nn.ModuleList([Block(self.embed_dim, self.num_heads, self.num_experts, self.max_seq_lenght) for _ in range(self.num_blocks)])\n",
        "        self.rmsnorm = RMSNorm(self.embed_dim)\n",
        "        self.output_linear = nn.Linear(self.embed_dim, self.vocab_size)\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        x = self.embedding(x)\n",
        "        for block in self.blocks:\n",
        "            x, top8_indicies, top8_probs, xj_logits = block(x)\n",
        "        output = self.rmsnorm(x)\n",
        "        output = self.output_linear(output)\n",
        "        return output, top8_indicies, top8_probs, xj_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HVeoRNnQWvB"
      },
      "outputs": [],
      "source": [
        "def load_balancing_loss(num_experts: int,\n",
        "                        topk_probs: torch.Tensor,      # [B, T, K]\n",
        "                        topk_indices: torch.Tensor,    # [B, T, K]\n",
        "                        alpha: float = 0.01):\n",
        "    B, T, K = topk_indices.shape\n",
        "    tot_tokens = B * T\n",
        "\n",
        "    # mask[b,t,k,e] == 1 if that (token,k) routes to expert e\n",
        "    mask = (topk_indices.unsqueeze(-1) ==\n",
        "            torch.arange(num_experts, device=topk_indices.device))\n",
        "\n",
        "    # f_i  –– fraction of tokens routed to expert i\n",
        "    tokens_per_expert = mask.any(dim=2).sum((0,1)).float()      # [E]\n",
        "    f = tokens_per_expert / tot_tokens\n",
        "\n",
        "    # P_i –– mean router prob mass arriving at expert i\n",
        "    probs_per_expert = (topk_probs.unsqueeze(-1) *\n",
        "                        mask.float()).sum((0,1,2))              # [E]\n",
        "    P = probs_per_expert / tot_tokens\n",
        "\n",
        "    lb_loss = alpha * num_experts * (f * P).sum()\n",
        "    return lb_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiybxUn6QYHZ"
      },
      "outputs": [],
      "source": [
        "def router_z_loss(router_logits, beta=0.001):\n",
        "    loss = torch.logsumexp(router_logits, dim=-1) ** 2\n",
        "    loss = torch.mean(loss)\n",
        "    return beta * loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev4WL9D4QY4Z"
      },
      "outputs": [],
      "source": [
        "# Helper function for detailed expert utilization logging\n",
        "def log_expert_utilization(top8_indices, top8_probs, num_experts, epoch):\n",
        "    \"\"\"\n",
        "    Log detailed expert utilization metrics and visualizations to wandb\n",
        "    \"\"\"\n",
        "\n",
        "    expert_counts = torch.bincount(top8_indices.view(-1), minlength=num_experts).float()\n",
        "    expert_usage_percentage = (expert_counts / expert_counts.sum()) * 100\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1.bar(range(num_experts), expert_counts.cpu().numpy())\n",
        "    ax1.set_xlabel('Expert ID')\n",
        "    ax1.set_ylabel('Number of Tokens Routed')\n",
        "    ax1.set_title(f'Expert Utilization Distribution (Epoch {epoch})')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    ax2.bar(range(num_experts), expert_usage_percentage.cpu().numpy())\n",
        "    ax2.set_xlabel('Expert ID')\n",
        "    ax2.set_ylabel('Percentage of Total Tokens (%)')\n",
        "    ax2.set_title(f'Expert Usage Percentage (Epoch {epoch})')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    wandb.log({\n",
        "        f\"expert_utilization/distribution_epoch_{epoch}\": wandb.Image(fig),\n",
        "        \"expert_utilization/expert_counts\": wandb.Histogram(expert_counts.cpu().numpy()),\n",
        "        \"expert_utilization/routing_probs\": wandb.Histogram(top8_probs.cpu().numpy()),\n",
        "        \"expert_utilization/std_dev\": torch.std(expert_counts).item(),\n",
        "        \"expert_utilization/coefficient_of_variation\": (torch.std(expert_counts) / torch.mean(expert_counts)).item(),\n",
        "    })\n",
        "\n",
        "    plt.close(fig)\n",
        "\n",
        "    return expert_counts, expert_usage_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_distributed(rank, world_size):\n",
        "    \"\"\"Initialize distributed training environment\"\"\"\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    \n",
        "    # Initialize the process group\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "    \n",
        "    # Set device for this process\n",
        "    torch.cuda.set_device(rank)\n",
        "    \n",
        "def cleanup_distributed():\n",
        "    \"\"\"Clean up distributed training environment\"\"\"\n",
        "    dist.destroy_process_group()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lr(it, warmup_steps, max_lr, max_steps, min_lr):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fxhD_oJQaUD"
      },
      "outputs": [],
      "source": [
        "def train_model(rank, world_size, num_blocks, batch_size, max_seq_lenght, vocab_size, num_heads, num_experts, grad_accum_steps, epochs, lr, embed_dim, checkpoint_epoch, warmup_steps, max_lr, max_steps, min_lr, data_root, project_name=\"mixture-of-experts\", run_name=None):\n",
        "    # Setup distributed training\n",
        "    setup_distributed(rank, world_size)\n",
        "    \n",
        "    device = torch.device(f\"cuda:{rank}\")\n",
        "    torch.cuda.set_device(device)\n",
        "    \n",
        "    print(f\"[blue]Process {rank}/{world_size} using device: {device}[/blue]\")\n",
        "    \n",
        "    # Initialize wandb only on rank 0\n",
        "    if rank == 0:\n",
        "        config = {\n",
        "            \"batch_size\": batch_size,\n",
        "            \"max_seq_length\": max_seq_lenght,\n",
        "            \"vocab_size\": vocab_size,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_experts\": num_experts,\n",
        "            \"grad_accum_steps\": grad_accum_steps,\n",
        "            \"epochs\": epochs,\n",
        "            \"learning_rate\": lr,\n",
        "            \"device\": str(device),\n",
        "            \"embed_dim\": embed_dim,\n",
        "            \"top_k_experts\": TOP_K_EXPERTS,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"warmup_steps\": warmup_steps,\n",
        "            \"max_lr\": max_lr,\n",
        "            \"max_steps\": max_steps,\n",
        "            \"min_lr\": min_lr,\n",
        "            \"world_size\": world_size\n",
        "        }\n",
        "        wandb.init(\n",
        "            project=project_name,\n",
        "            name=run_name,\n",
        "            config=config,\n",
        "            tags=[\"mixture-of-experts\", \"transformer\", \"moe\", \"cuda\", \"distributed\"]\n",
        "        )\n",
        "    \n",
        "    # Calculate per-GPU batch size\n",
        "    per_gpu_batch_size = max(1, batch_size // world_size)\n",
        "    \n",
        "    # Create dataloaders with distributed sampling\n",
        "    train_dataloader = DataLoader(per_gpu_batch_size, max_seq_lenght, \"train\", data_root, rank, world_size)\n",
        "    val_dataloader = DataLoader(per_gpu_batch_size, max_seq_lenght, \"val\", data_root, rank, world_size)\n",
        "    \n",
        "    # Create model and move to GPU\n",
        "    model = Model(vocab_size, embed_dim, max_seq_lenght, num_heads, num_experts, num_blocks=num_blocks).to(device)\n",
        "    \n",
        "    # Wrap model with DDP\n",
        "    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)\n",
        "    \n",
        "    # Log model parameters only on rank 0\n",
        "    if rank == 0:\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        wandb.config.update({\n",
        "            \"total_parameters\": total_params,\n",
        "            \"trainable_parameters\": trainable_params,\n",
        "            \"per_gpu_batch_size\": per_gpu_batch_size\n",
        "        })\n",
        "        print(f\"[green]Model parameters: {total_params:,} total, {trainable_params:,} trainable[/green]\")\n",
        "        print(f\"[blue]Per-GPU batch size: {per_gpu_batch_size}[/blue]\")\n",
        "    \n",
        "    # Setup optimizer with fused AdamW if available\n",
        "    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "    use_fused = fused_available and torch.cuda.is_available()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.1, fused=use_fused)\n",
        "    \n",
        "    # Setup gradient scaler for mixed precision\n",
        "    scaler = GradScaler()\n",
        "    \n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss_acum = 0.0\n",
        "        train_lb_loss_acum = 0.0\n",
        "        train_rz_loss_acum = 0.0\n",
        "        train_ce_loss_acum = 0.0\n",
        "        \n",
        "        for micro_step in range(grad_accum_steps):\n",
        "            x, y = train_dataloader.next_batch()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            \n",
        "            # Use autocast for mixed precision\n",
        "            with autocast():\n",
        "                output, top8_indicies, top8_probs, xj_logits = model(x)\n",
        "                train_lb_loss = load_balancing_loss(num_experts, top8_probs, top8_indicies)\n",
        "                train_rz_loss = router_z_loss(xj_logits)\n",
        "                train_ce_loss = loss_fn(output.view(-1, output.size(-1)), y.view(-1))\n",
        "                train_loss = train_ce_loss + train_lb_loss + train_rz_loss\n",
        "                \n",
        "            train_loss = train_loss / grad_accum_steps\n",
        "            loss_acum += train_loss.detach()\n",
        "            train_lb_loss_acum += train_lb_loss.detach() / grad_accum_steps\n",
        "            train_rz_loss_acum += train_rz_loss.detach() / grad_accum_steps\n",
        "            train_ce_loss_acum += train_ce_loss.detach() / grad_accum_steps\n",
        "            \n",
        "            # Backward pass with gradient scaling\n",
        "            scaler.scale(train_loss).backward()\n",
        "        \n",
        "        # Gradient clipping and optimizer step\n",
        "        scaler.unscale_(optimizer)\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        # Update learning rate\n",
        "        current_lr = get_lr(epoch, warmup_steps, max_lr, max_steps, min_lr)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_lr\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            x, y = val_dataloader.next_batch()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            \n",
        "            with autocast():\n",
        "                output, top8_indicies, top8_probs, xj_logits = model(x)\n",
        "                val_lb_loss = load_balancing_loss(num_experts, top8_probs, top8_indicies)\n",
        "                val_rz_loss = router_z_loss(xj_logits)\n",
        "                val_ce_loss = loss_fn(output.view(-1, output.size(-1)), y.view(-1))\n",
        "                val_loss = val_ce_loss + val_lb_loss + val_rz_loss\n",
        "        \n",
        "        # Save checkpoint on rank 0\n",
        "        if epoch % checkpoint_epoch == 0 and rank == 0:\n",
        "            os.makedirs(\"./weights\", exist_ok=True)\n",
        "            torch.save(model.module.state_dict(), f\"./weights/model-{run_name}.pth\")\n",
        "            torch.save(optimizer.state_dict(), f\"./weights/optimizer-{run_name}.pth\")\n",
        "        \n",
        "        # Calculate expert utilization metrics\n",
        "        expert_counts = torch.bincount(top8_indicies.view(-1), minlength=num_experts).float()\n",
        "        expert_utilization = (expert_counts > 0).sum().item() / num_experts\n",
        "        expert_load_variance = torch.var(expert_counts).item()\n",
        "        \n",
        "        # Synchronize metrics across all processes\n",
        "        if world_size > 1:\n",
        "            # Reduce losses across all processes\n",
        "            dist.all_reduce(loss_acum, op=dist.ReduceOp.SUM)\n",
        "            dist.all_reduce(val_loss, op=dist.ReduceOp.SUM)\n",
        "            dist.all_reduce(train_ce_loss_acum, op=dist.ReduceOp.SUM)\n",
        "            dist.all_reduce(val_ce_loss, op=dist.ReduceOp.SUM)\n",
        "            \n",
        "            loss_acum /= world_size\n",
        "            val_loss /= world_size\n",
        "            train_ce_loss_acum /= world_size\n",
        "            val_ce_loss /= world_size\n",
        "        \n",
        "        # Log metrics only on rank 0\n",
        "        if rank == 0:\n",
        "            wandb.log({\n",
        "                \"train/total_loss\": loss_acum.item(),\n",
        "                \"train/cross_entropy_loss\": train_ce_loss_acum.item(),\n",
        "                \"train/load_balancing_loss\": train_lb_loss_acum.item(),\n",
        "                \"train/router_z_loss\": train_rz_loss_acum.item(),\n",
        "                \"val/total_loss\": val_loss.item(),\n",
        "                \"val/cross_entropy_loss\": val_ce_loss.item(),\n",
        "                \"val/load_balancing_loss\": val_lb_loss.item(),\n",
        "                \"val/router_z_loss\": val_rz_loss.item(),\n",
        "                \"experts/utilization_rate\": expert_utilization,\n",
        "                \"experts/load_variance\": expert_load_variance,\n",
        "                \"experts/mean_routing_prob\": top8_probs.mean().item(),\n",
        "                \"experts/max_routing_prob\": top8_probs.max().item(),\n",
        "                \"experts/min_routing_prob\": top8_probs.min().item(),\n",
        "                \"training/epoch\": epoch,\n",
        "                \"training/learning_rate\": current_lr,\n",
        "                \"training/gradient_norm\": grad_norm.item(),\n",
        "                \"train/perplexity\": torch.exp(train_ce_loss_acum).item(),\n",
        "                \"val/perplexity\": torch.exp(val_ce_loss).item(),\n",
        "            })\n",
        "            \n",
        "            print(f\"[purple]Epoch[/purple]: {epoch}| [blue]Train Loss[/blue]: {loss_acum.item():.4f} | [magenta]Val Loss[/magenta]: {val_loss.item():.4f} | [green]Expert Util[/green]: {expert_utilization:.3f} | [bold turquoise4]lr[/bold turquoise4]: {current_lr}\")\n",
        "            \n",
        "            # Log expert utilization visualization periodically\n",
        "            if epoch % 10 == 0:\n",
        "                try:\n",
        "                    log_expert_utilization(top8_indicies, top8_probs, num_experts, epoch)\n",
        "                except Exception as e:\n",
        "                    print(f\"[yellow]Warning: Could not log expert utilization visualization: {e}[/yellow]\")\n",
        "    \n",
        "    # Clean up\n",
        "    cleanup_distributed()\n",
        "    \n",
        "    if rank == 0:\n",
        "        wandb.finish()\n",
        "\n",
        "def train(num_blocks, batch_size, max_seq_lenght, vocab_size, num_heads, num_experts, grad_accum_steps, epochs, lr, embed_dim, checkpoint_epoch, warmup_steps, max_lr, max_steps, min_lr, data_root, project_name=\"mixture-of-experts\", run_name=None):\n",
        "    \"\"\"Main training function that spawns distributed CUDA training\"\"\"\n",
        "    \n",
        "    # Check CUDA availability\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA is not available. This training script requires NVIDIA GPUs.\")\n",
        "    \n",
        "    world_size = torch.cuda.device_count()\n",
        "    print(f\"[green]Starting distributed training on {world_size} CUDA devices[/green]\")\n",
        "    \n",
        "    # Spawn distributed training processes\n",
        "    mp.spawn(\n",
        "        train_model,\n",
        "        args=(world_size, num_blocks, batch_size, max_seq_lenght, vocab_size, num_heads, num_experts, grad_accum_steps, epochs, lr, embed_dim, checkpoint_epoch, warmup_steps, max_lr, max_steps, min_lr, data_root, project_name, run_name),\n",
        "        nprocs=world_size,\n",
        "        join=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "Hx9xmru5Qiq2",
        "outputId": "b39b0eab-ad76-4ee9-ec6d-78487d8ec8d5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">found </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">99</span><span style=\"color: #008000; text-decoration-color: #008000\"> shards for split train</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32mfound \u001b[0m\u001b[1;32m99\u001b[0m\u001b[32m shards for split train\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">found </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\"> shards for split val</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32mfound \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m shards for split val\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "Bad StatusOr access: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 128.00M. That was not possible. There are 38.52M free.; (0x0x0_HBM0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-3943040359.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mCHECKPOINT_EPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     train(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmax_seq_lenght\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-19-9700013.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(batch_size, max_seq_lenght, vocab_size, num_heads, num_experts, grad_accum_steps, epochs, lr, device, embed_dim, checkpoint_epoch, project_name, run_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_lenght\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_lenght\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_experts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Bad StatusOr access: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 128.00M. That was not possible. There are 38.52M free.; (0x0x0_HBM0)"
          ]
        }
      ],
      "source": [
        "# Training configuration\n",
        "BATCH_SIZE = 128  # Total batch size across all GPUs\n",
        "MAX_SEQ_LENGTH = 4096\n",
        "VOCAB_SIZE = 50304\n",
        "NUM_HEADS = 16\n",
        "NUM_EXPERTS = 64\n",
        "GRAD_ACCUM_STEPS = 4\n",
        "EPOCHS = 10000\n",
        "LEARNING_RATE = 3e-4\n",
        "EMBED_DIM = 1024\n",
        "CHECKPOINT_EPOCH = 100\n",
        "NUM_BLOCKS = 16\n",
        "WARMUP_STEPS = 750\n",
        "MAX_LR = LEARNING_RATE\n",
        "MIN_LR = LEARNING_RATE * 0.1\n",
        "DATA_ROOT = \"./data\"  # Update this path to your data location\n",
        "\n",
        "# Start training\n",
        "train(\n",
        "    num_blocks=NUM_BLOCKS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_seq_lenght=MAX_SEQ_LENGTH,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_experts=NUM_EXPERTS,\n",
        "    grad_accum_steps=GRAD_ACCUM_STEPS,\n",
        "    epochs=EPOCHS,\n",
        "    lr=LEARNING_RATE,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    checkpoint_epoch=CHECKPOINT_EPOCH,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    max_lr=MAX_LR,\n",
        "    max_steps=EPOCHS,\n",
        "    min_lr=MIN_LR,\n",
        "    data_root=DATA_ROOT,\n",
        "    project_name=\"mixture-of-experts-cuda\",\n",
        "    run_name=f\"moe-{NUM_EXPERTS}experts-{NUM_HEADS}heads-{EMBED_DIM}dim-cuda\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o596o2oMtHYN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
