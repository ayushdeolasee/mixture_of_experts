{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGHT = 4096\n",
    "VOCAB_SIZE = 50304\n",
    "EMBEDDING_DIM = 1024\n",
    "NUM_HEADS = 16\n",
    "NUM_BLOCKS = 16\n",
    "BATCH_SIZE = 128\n",
    "NUM_EXPERTS = 64\n",
    "TOP_K_EXPERTS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, B, T, split, data_root):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        data_root = data_root\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        if (len(shards) > 0) == False:\n",
    "            print(f\"no shards found for split {split}\")\n",
    "        # assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "        print(f\"[green]found {len(shards)} shards for split {split}[/green]\")\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard zero\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_lenght):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_lenght\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        pos = torch.arange(self.max_seq_len).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", pos, inv_freq)\n",
    "        self.cos = torch.cos(freqs)\n",
    "        self.sin = torch.sin(freqs)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        self.seq_len = x.size(2)\n",
    "        x1 = x[..., ::2]\n",
    "        x2 = x[..., 1::2]\n",
    "        cos = self.cos[:self.seq_len].unsqueeze(0).to(x.device).view(list(x1.size()))\n",
    "        sin = self.sin[:self.seq_len].unsqueeze(0).to(x.device).view(list(x1.size()))\n",
    "        x_rotated = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        \n",
    "        return x_rotated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_dimension):\n",
    "        super().__init__()\n",
    "        # First linear layer outputs 2 * hidden_dimension for the gate and value\n",
    "        self.linear1 = nn.Linear(input_dimension, 2 * hidden_dimension, bias=True)\n",
    "        # Second linear layer takes hidden_dimension and outputs input_dimension\n",
    "        self.linear2 = nn.Linear(hidden_dimension, input_dimension, bias=True)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        combined = self.linear1(x)\n",
    "        a, b = combined.chunk(2, dim=-1)\n",
    "        swish = b * torch.sigmoid(b)\n",
    "        output = self.linear2(swish * a)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, input_shape, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(input_shape))\n",
    "        self.b = nn.Parameter(torch.ones(input_shape))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        output = x / rms \n",
    "        output = (output * self.g) + self.b\n",
    "        return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, max_seq_lenght, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.max_seq_lenght = max_seq_lenght \n",
    "        self.eps = eps \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd)\n",
    "        self.alpha = nn.Parameter(torch.ones(self.n_head))\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        self.rope_q = RotaryPositionEmbedding(self.n_embd, self.max_seq_lenght) \n",
    "        self.rope_k = RotaryPositionEmbedding(self.n_embd, self.max_seq_lenght)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, number of heads, T, head_size)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, numebr of heads, T, head_size)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, number_of_heads, T, head_size)\n",
    "        q_norm = torch.norm(q, dim=-1, keepdim=True)\n",
    "        k_norm = torch.norm(k, dim=-1, keepdim=True)\n",
    "        q_hat = q / (q_norm + self.eps)\n",
    "        k_hat = k / (k_norm + self.eps)\n",
    "\n",
    "        factor = self.alpha * math.sqrt(C // self.n_head)\n",
    "        factor = factor.view(1, self.n_head, 1, 1)\n",
    "        q_scaled = q_hat * factor\n",
    "         \n",
    "        q_scaled = self.rope_q(q_scaled)\n",
    "        k_hat = self.rope_k(q_scaled)\n",
    "        y = F.scaled_dot_product_attention(q_scaled, k_hat, v, is_causal=True, dropout_p=0.0)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 256])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Batch, number of heads, sequence lenght, head size)\n",
    "\n",
    "input = torch.randn([1, 128, 256])\n",
    "attention = CausalSelfAttention(256, 4, input.size(1))\n",
    "output = attention(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.expert = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            SwiGLU(4 * embed_dim, 4 * embed_dim),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return self.expert(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(nn.Module):\n",
    "    def __init__(self, num_experts, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.router = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.num_experts),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.router(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn([1, 128, 256])\n",
    "# router = Router(12, 128)\n",
    "# router(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_experts, max_seq_lenght):\n",
    "        super(Block, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_experts = num_experts\n",
    "        self.max_seq_lenght = max_seq_lenght\n",
    "\n",
    "        self.RMSNorm = RMSNorm(self.embed_dim)\n",
    "        self.MultiheadAttention = CausalSelfAttention(self.embed_dim, self.num_heads, self.max_seq_lenght)\n",
    "        self.router = Router(self.num_experts, self.embed_dim)\n",
    "        self.experts = nn.ModuleList([Expert(self.embed_dim) for _ in range(self.num_experts)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.MultiheadAttention(self.RMSNorm(x))\n",
    "        \n",
    "        routes = self.router(x)\n",
    "        top8_probs, top8_indices = torch.topk(routes, k=8, dim=2) \n",
    "        top8_probs = top8_probs / top8_probs.sum(dim=-1, keepdim=True)\n",
    "        expert_output = torch.zeros_like(x)\n",
    "        for k in range(8):\n",
    "            expert_idx = top8_indices[:, :, k]\n",
    "            prob = top8_probs[:, :, k]\n",
    "            for expert_id in range(self.num_experts):\n",
    "                mask = (expert_idx == expert_id)\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                x_selected = x[mask]\n",
    "                expert_out = self.experts[expert_id](x_selected)\n",
    "                prob_selected = prob[mask].unsqueeze(-1)\n",
    "                weighted_out = expert_out * prob_selected\n",
    "                expert_output[mask] += weighted_out\n",
    "        \n",
    "        x = x + expert_output\n",
    "        \n",
    "        return x, top8_indices, top8_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_lenght, num_heads, num_experts):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_lenght = max_seq_lenght\n",
    "        self.num_heads = num_heads\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_dim, dtype=torch.float32).to(device)\n",
    "        \n",
    "        self.blocks = Block(self.embed_dim, self.num_heads, self.num_experts, self.max_seq_lenght)\n",
    "        self.rmsnorm = RMSNorm(self.embed_dim)\n",
    "        self.output_linear = nn.Linear(self.embed_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        x = self.embedding(x)\n",
    "        x_debug, top8_indicies, top8_probs = self.blocks(x)\n",
    "        x += x_debug\n",
    "        output = self.rmsnorm(x)\n",
    "        output = self.output_linear(output)\n",
    "\n",
    "        return output, top8_indicies, top8_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balancing_loss(num_experts: int,\n",
    "                        topk_probs: torch.Tensor,      # [B, T, K]\n",
    "                        topk_indices: torch.Tensor,    # [B, T, K]\n",
    "                        alpha: float = 0.01):\n",
    "    B, T, K = topk_indices.shape\n",
    "    tot_tokens = B * T\n",
    "\n",
    "    # mask[b,t,k,e] == 1 if that (token,k) routes to expert e\n",
    "    mask = (topk_indices.unsqueeze(-1) ==\n",
    "            torch.arange(num_experts, device=topk_indices.device))\n",
    "\n",
    "    # f_i  –– fraction of tokens routed to expert i\n",
    "    tokens_per_expert = mask.any(dim=2).sum((0,1)).float()      # [E]\n",
    "    f = tokens_per_expert / tot_tokens\n",
    "\n",
    "    # P_i –– mean router prob mass arriving at expert i\n",
    "    probs_per_expert = (topk_probs.unsqueeze(-1) *\n",
    "                        mask.float()).sum((0,1,2))              # [E]\n",
    "    P = probs_per_expert / tot_tokens\n",
    "\n",
    "    lb_loss = alpha * num_experts * (f * P).sum()\n",
    "    return lb_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixture_of_experts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
