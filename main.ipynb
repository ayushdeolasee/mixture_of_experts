{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpypdeveloper\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import inspect\n",
    "from rich import print\n",
    "import logging\n",
    "from rich.logging import RichHandler\n",
    "import wandb\n",
    "import torchinfo\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT = \"%(message)s\"\n",
    "logging.basicConfig(\n",
    "    level=\"NOTSET\", format=FORMAT, datefmt=\"[%X]\", handlers=[RichHandler()]\n",
    ")\n",
    "\n",
    "log = logging.getLogger(\"rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Using device: mps</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mUsing device: mps\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"[green]Using device: {device}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGHT = 4096\n",
    "VOCAB_SIZE = 50304\n",
    "EMBEDDING_DIM = 1024\n",
    "NUM_HEADS = 16\n",
    "NUM_BLOCKS = 16\n",
    "BATCH_SIZE = 128\n",
    "NUM_EXPERTS = 64\n",
    "TOP_K_EXPERTS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, B, T, split, data_root):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        data_root = data_root\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        if (len(shards) > 0) == False:\n",
    "            print(f\"no shards found for split {split}\")\n",
    "        # assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "        print(f\"[green]found {len(shards)} shards for split {split}[/green]\")\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard zero\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_length=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        position = torch.arange(max_seq_length).float()\n",
    "        freqs = torch.einsum('i,j->ij', position, inv_freq)\n",
    "        self.register_buffer('cos_cached', torch.cos(freqs), persistent=False)\n",
    "        self.register_buffer('sin_cached', torch.sin(freqs), persistent=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(-2)\n",
    "        cos = self.cos_cached[:seq_len]\n",
    "        sin = self.sin_cached[:seq_len]\n",
    "        x_rotated = self.apply_rotary_pos_emb(x, cos, sin)\n",
    "        return x_rotated\n",
    "    \n",
    "    def apply_rotary_pos_emb(self, x, cos, sin):\n",
    "        x1 = x[..., ::2]\n",
    "        x2 = x[..., 1::2]\n",
    "        cos = cos.unsqueeze(0).unsqueeze(0)\n",
    "        sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "        x1_rotated = x1 * cos - x2 * sin\n",
    "        x2_rotated = x1 * sin + x2 * cos\n",
    "        x_rotated = torch.stack([x1_rotated, x2_rotated], dim=-1)\n",
    "        x_rotated = x_rotated.flatten(start_dim=-2)\n",
    "        return x_rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_dimension):\n",
    "        super().__init__()\n",
    "        # First linear layer outputs 2 * hidden_dimension for the gate and value\n",
    "        self.linear1 = nn.Linear(input_dimension, 2 * hidden_dimension, bias=True)\n",
    "        # Second linear layer takes hidden_dimension and outputs input_dimension\n",
    "        self.linear2 = nn.Linear(hidden_dimension, input_dimension, bias=True)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        combined = self.linear1(x)\n",
    "        a, b = combined.chunk(2, dim=-1)\n",
    "        swish = b * torch.sigmoid(b)\n",
    "        output = self.linear2(swish * a)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, input_shape, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(input_shape))\n",
    "        self.b = nn.Parameter(torch.ones(input_shape))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        output = x / rms \n",
    "        output = (output * self.g) + self.b\n",
    "        return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, max_seq_lenght, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        self.max_seq_lenght = max_seq_lenght \n",
    "        self.eps = eps \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd)\n",
    "        self.alpha = nn.Parameter(torch.ones(self.n_head))\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        self.rope = RotaryPositionEmbedding(self.head_dim, self.max_seq_lenght) \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, number of heads, T, head_size)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, numebr of heads, T, head_size)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, number_of_heads, T, head_size)\n",
    "        q_norm = torch.norm(q, dim=-1, keepdim=True)\n",
    "        k_norm = torch.norm(k, dim=-1, keepdim=True)\n",
    "        q_hat = q / (q_norm + self.eps)\n",
    "        k_hat = k / (k_norm + self.eps)\n",
    "        \n",
    "        factor = self.alpha * math.sqrt(C // self.n_head)\n",
    "        factor = factor.view(1, self.n_head, 1, 1)\n",
    "        q_scaled = q_hat * factor\n",
    "        q_scaled = self.rope(q_scaled)\n",
    "        k_hat = self.rope(k_hat)\n",
    "        y = F.scaled_dot_product_attention(q_scaled, k_hat, v, is_causal=True, dropout_p=0.0)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.expert = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 2 * embed_dim),\n",
    "            SwiGLU(2 * embed_dim, 2 * embed_dim),\n",
    "            nn.Linear(2 * embed_dim, embed_dim),\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return self.expert(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(nn.Module):\n",
    "    def __init__(self, num_experts, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear = nn.Linear(self.embed_dim, self.num_experts)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x): \n",
    "        logits = self.linear(x)\n",
    "        output = self.softmax(logits)\n",
    "        return output, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_experts, max_seq_lenght, top_k):\n",
    "        super(Block, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_experts = num_experts\n",
    "        self.max_seq_lenght = max_seq_lenght\n",
    "        self.top_k = top_k\n",
    "\n",
    "        self.RMSNorm = RMSNorm(self.embed_dim)\n",
    "        self.MultiheadAttention = CausalSelfAttention(self.embed_dim, self.num_heads, self.max_seq_lenght)\n",
    "        self.router = Router(self.num_experts, self.embed_dim)\n",
    "        self.experts = nn.ModuleList([Expert(self.embed_dim) for _ in range(self.num_experts)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.MultiheadAttention(self.RMSNorm(x))\n",
    "        routes, xj_logits = self.router(x)\n",
    "        top8_probs, top8_indices = torch.topk(routes, k=self.top_k, dim=2) \n",
    "        top8_probs = top8_probs / top8_probs.sum(dim=-1, keepdim=True)\n",
    "        expert_output = torch.zeros_like(x)\n",
    "        for k in range(8):\n",
    "            expert_idx = top8_indices[:, :, k]\n",
    "            prob = top8_probs[:, :, k]\n",
    "            for expert_id in range(self.num_experts):\n",
    "                mask = (expert_idx == expert_id)\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                x_selected = x[mask]\n",
    "                expert_out = self.experts[expert_id](x_selected)\n",
    "                prob_selected = prob[mask].unsqueeze(-1)\n",
    "                weighted_out = expert_out * prob_selected\n",
    "                expert_output[mask] += weighted_out\n",
    "        \n",
    "        x = x + expert_output\n",
    "        \n",
    "        return x, top8_indices, top8_probs, xj_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert blocks to a ModuleList look at tpu_train.ipynb for reference!\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_lenght, num_heads, num_experts, top_k, num_blocks=16):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_lenght = max_seq_lenght\n",
    "        self.num_heads = num_heads\n",
    "        self.num_experts = num_experts\n",
    "        self.num_blocks = num_blocks\n",
    "        self.top_k = top_k\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_dim, dtype=torch.float32).to(device)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([Block(self.embed_dim, self.num_heads, self.num_experts, self.max_seq_lenght, self.top_k) for _ in range(self.num_blocks)])\n",
    "        self.rmsnorm = RMSNorm(self.embed_dim)\n",
    "        self.output_linear = nn.Linear(self.embed_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        x = self.embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x, top8_indicies, top8_probs, xj_logits = block(x)\n",
    "        output = self.rmsnorm(x)\n",
    "        output = self.output_linear(output)\n",
    "\n",
    "        return output, top8_indicies, top8_probs, xj_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "MAX_SEQ_LENGTH = 1024 \n",
    "WARMUP_STEPS = 750\n",
    "MIN_LR = 3e-6\n",
    "MAX_LR = 3e-4\n",
    "VOCAB_SIZE = 25154\n",
    "NUM_HEADS = 8 \n",
    "NUM_EXPERTS = 16\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "EPOCHS = 10000\n",
    "LEARNING_RATE = 3e-4\n",
    "EMBED_DIM = 1024\n",
    "CHECKPOINT_EPOCH = 10\n",
    "TOP_K = 4\n",
    "NUM_BLOCKS = 8 \n",
    "\n",
    "model = Model(VOCAB_SIZE, EMBED_DIM, MAX_SEQ_LENGTH, NUM_HEADS, NUM_EXPERTS, TOP_K, NUM_BLOCKS).to(device)\n",
    "num_params = sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7603698434</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m7603698434\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balancing_loss(num_experts: int,\n",
    "                        topk_probs: torch.Tensor,      # [B, T, K]\n",
    "                        topk_indices: torch.Tensor,    # [B, T, K]\n",
    "                        alpha: float = 0.01):\n",
    "    B, T, K = topk_indices.shape\n",
    "    tot_tokens = B * T\n",
    "\n",
    "    # mask[b,t,k,e] == 1 if that (token,k) routes to expert e\n",
    "    mask = (topk_indices.unsqueeze(-1) ==\n",
    "            torch.arange(num_experts, device=topk_indices.device))\n",
    "\n",
    "    # f_i  –– fraction of tokens routed to expert i\n",
    "    tokens_per_expert = mask.any(dim=2).sum((0,1)).float()      # [E]\n",
    "    f = tokens_per_expert / tot_tokens\n",
    "\n",
    "    # P_i –– mean router prob mass arriving at expert i\n",
    "    probs_per_expert = (topk_probs.unsqueeze(-1) *\n",
    "                        mask.float()).sum((0,1,2))              # [E]\n",
    "    P = probs_per_expert / tot_tokens\n",
    "\n",
    "    lb_loss = alpha * num_experts * (f * P).sum()\n",
    "    return lb_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_z_loss(router_logits, beta=0.001):\n",
    "    loss = torch.logsumexp(router_logits, dim=-1) ** 2 \n",
    "    loss = torch.mean(loss)\n",
    "    return beta * loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for detailed expert utilization logging\n",
    "def log_expert_utilization(top8_indices, top8_probs, num_experts, epoch):\n",
    "    \"\"\"\n",
    "    Log detailed expert utilization metrics and visualizations to wandb\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate expert usage counts\n",
    "    expert_counts = torch.bincount(top8_indices.view(-1), minlength=num_experts).float()\n",
    "    expert_usage_percentage = (expert_counts / expert_counts.sum()) * 100\n",
    "    \n",
    "    # Create expert utilization histogram\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Expert usage distribution\n",
    "    ax1.bar(range(num_experts), expert_counts.cpu().numpy())\n",
    "    ax1.set_xlabel('Expert ID')\n",
    "    ax1.set_ylabel('Number of Tokens Routed')\n",
    "    ax1.set_title(f'Expert Utilization Distribution (Epoch {epoch})')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Expert usage percentage\n",
    "    ax2.bar(range(num_experts), expert_usage_percentage.cpu().numpy())\n",
    "    ax2.set_xlabel('Expert ID')\n",
    "    ax2.set_ylabel('Percentage of Total Tokens (%)')\n",
    "    ax2.set_title(f'Expert Usage Percentage (Epoch {epoch})')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        f\"expert_utilization/distribution_epoch_{epoch}\": wandb.Image(fig),\n",
    "        \"expert_utilization/expert_counts\": wandb.Histogram(expert_counts.cpu().numpy()),\n",
    "        \"expert_utilization/routing_probs\": wandb.Histogram(top8_probs.cpu().numpy()),\n",
    "        \"expert_utilization/std_dev\": torch.std(expert_counts).item(),\n",
    "        \"expert_utilization/coefficient_of_variation\": (torch.std(expert_counts) / torch.mean(expert_counts)).item(),\n",
    "    })\n",
    "    \n",
    "    plt.close(fig)\n",
    "    \n",
    "    return expert_counts, expert_usage_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it, warmup_steps, max_lr, max_steps, min_lr):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, max_seq_lenght, vocab_size, num_heads, num_experts, grad_accum_steps, epochs, lr, device, embed_dim, checkpoint_epoch, warmup_steps, max_lr, max_steps, min_lr, top_k, num_blocks, project_name=\"mixture-of-experts\", run_name=None):\n",
    "    config = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_seq_length\": max_seq_lenght,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"num_experts\": num_experts,\n",
    "        \"grad_accum_steps\": grad_accum_steps,\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"device\": device,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"top_k_experts\": top_k,\n",
    "        \"num_blocks\": num_blocks, \n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"max_lr\": max_lr,\n",
    "        \"max_steps\": max_steps,\n",
    "        \"min_lr\": min_lr, \n",
    "    }\n",
    "    \n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        config=config,\n",
    "        tags=[\"mixture-of-experts\", \"transformer\", \"moe\"]\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(B=batch_size, T=max_seq_lenght, split=\"train\", data_root=\"./data\")\n",
    "    val_dataloader = DataLoader(B=batch_size, T=max_seq_lenght, split=\"val\", data_root=\"./data\")\n",
    "\n",
    "    model = Model(vocab_size, embed_dim, max_seq_lenght, num_heads, num_experts, top_k, num_blocks).to(device)\n",
    "    \n",
    "    # Log model parameters count\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    wandb.config.update({\n",
    "        \"total_parameters\": total_params,\n",
    "        \"trainable_parameters\": trainable_params\n",
    "    })\n",
    "    \n",
    "    if device != \"mps\":\n",
    "        model = torch.compile(model)\n",
    "        print(\"Using compiled model\")\n",
    "\n",
    "    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "    use_fused = fused_available and device == \"cuda\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, fused=use_fused)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Watch model for gradient tracking\n",
    "    wandb.watch(model, log=\"all\", log_freq=100)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss_acum = 0.0\n",
    "        train_lb_loss_acum = 0.0\n",
    "        train_rz_loss_acum = 0.0\n",
    "        train_ce_loss_acum = 0.0\n",
    "        \n",
    "        for micro_step in range(grad_accum_steps):\n",
    "            x, y = train_dataloader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                output, top8_indicies, top8_probs, xj_logits = model(x)\n",
    "                train_lb_loss = load_balancing_loss(num_experts, top8_probs, top8_indicies)\n",
    "                train_rz_loss = router_z_loss(xj_logits)\n",
    "                train_ce_loss = loss_fn(output.view(-1, output.size(-1)), y.view(-1))\n",
    "                train_loss = train_ce_loss + train_lb_loss + train_rz_loss\n",
    "            \n",
    "            train_loss = train_loss / grad_accum_steps\n",
    "            loss_acum += train_loss.detach()\n",
    "            train_lb_loss_acum += train_lb_loss.detach() / grad_accum_steps\n",
    "            train_rz_loss_acum += train_rz_loss.detach() / grad_accum_steps\n",
    "            train_ce_loss_acum += train_ce_loss.detach() / grad_accum_steps\n",
    "            \n",
    "            train_loss.backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        lr = get_lr(epoch, warmup_steps, max_lr, max_steps, min_lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x, y = val_dataloader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                output, top8_indicies, top8_probs, xj_logits = model(x)\n",
    "                val_lb_loss = load_balancing_loss(num_experts, top8_probs, top8_indicies)\n",
    "                val_rz_loss = router_z_loss(xj_logits)\n",
    "                val_ce_loss = loss_fn(output.view(-1, output.size(-1)), y.view(-1))\n",
    "                val_loss = val_ce_loss + val_lb_loss + val_rz_loss\n",
    "        \n",
    "        if epoch % checkpoint_epoch == 0:\n",
    "            torch.save(model.state_dict(), f\"./weights/model-{run_name}.pth\")\n",
    "            torch.save(optimizer.state_dict(), f\"./weights/optimizer-{run_name}.pth\")\n",
    "        \n",
    "        # Calculate expert utilization metrics\n",
    "        expert_counts = torch.bincount(top8_indicies.view(-1), minlength=num_experts).float()\n",
    "        expert_utilization = (expert_counts > 0).sum().item() / num_experts\n",
    "        expert_load_variance = torch.var(expert_counts).item()\n",
    "\n",
    "        wandb.log({\n",
    "            # Training losses\n",
    "            \"train/total_loss\": loss_acum.item(),\n",
    "            \"train/cross_entropy_loss\": train_ce_loss_acum.item(),\n",
    "            \"train/load_balancing_loss\": train_lb_loss_acum.item(),\n",
    "            \"train/router_z_loss\": train_rz_loss_acum.item(),\n",
    "            \n",
    "            # Validation losses\n",
    "            \"val/total_loss\": val_loss.item(),\n",
    "            \"val/cross_entropy_loss\": val_ce_loss.item(),\n",
    "            \"val/load_balancing_loss\": val_lb_loss.item(),\n",
    "            \"val/router_z_loss\": val_rz_loss.item(),\n",
    "            \n",
    "            # Expert utilization metrics\n",
    "            \"experts/utilization_rate\": expert_utilization,\n",
    "            \"experts/load_variance\": expert_load_variance,\n",
    "            \"experts/mean_routing_prob\": top8_probs.mean().item(),\n",
    "            \"experts/max_routing_prob\": top8_probs.max().item(),\n",
    "            \"experts/min_routing_prob\": top8_probs.min().item(),\n",
    "            \n",
    "            # Training metrics\n",
    "            \"training/epoch\": epoch,\n",
    "            \"training/learning_rate\": lr,\n",
    "            \"training/gradient_norm\": grad_norm.item(),\n",
    "            \n",
    "            # Perplexity\n",
    "            \"train/perplexity\": torch.exp(train_ce_loss_acum).item(),\n",
    "            \"val/perplexity\": torch.exp(val_ce_loss).item(),\n",
    "        })\n",
    "        \n",
    "        print(f\"[purple]Epoch[/purple]: {epoch}| [blue]Train Loss[/blue]: {loss_acum.item():.4f} | [magenta]Val Loss[/magenta]: {val_loss.item():.4f} | [green]Expert Util[/green]: {expert_utilization:.3f} | [bold turquoise4]lr[/bold turquoise4]: {lr}\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            try:\n",
    "                log_expert_utilization(top8_indicies, top8_probs, num_experts, epoch)\n",
    "            except Exception as e:\n",
    "                print(f\"[yellow]Warning: Could not log expert utilization visualization: {e}[/yellow]\")\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:35:17] </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Popen</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008000; text-decoration-color: #008000\">'git'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cat-file'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'--batch-check'</span><span style=\"font-weight: bold\">]</span>,                                         <a href=\"file:///opt/homebrew/Caskroom/miniconda/base/envs/mixture_of_experts/lib/python3.11/site-packages/git/cmd.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">cmd.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/homebrew/Caskroom/miniconda/base/envs/mixture_of_experts/lib/python3.11/site-packages/git/cmd.py#1253\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1253</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #808000; text-decoration-color: #808000\">cwd</span>=<span style=\"color: #800080; text-decoration-color: #800080\">/Users/ayushdeolasee/Developer/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">mixture_of_experts</span>, <span style=\"color: #808000; text-decoration-color: #808000\">stdin</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">valid</span><span style=\"color: #000000; text-decoration-color: #000000\"> stream</span><span style=\"font-weight: bold\">&gt;</span>,        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #808000; text-decoration-color: #808000\">shell</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">universal_newlines</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:35:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m \u001b[1;35mPopen\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'git'\u001b[0m, \u001b[32m'cat-file'\u001b[0m, \u001b[32m'--batch-check'\u001b[0m\u001b[1m]\u001b[0m,                                         \u001b]8;id=416690;file:///opt/homebrew/Caskroom/miniconda/base/envs/mixture_of_experts/lib/python3.11/site-packages/git/cmd.py\u001b\\\u001b[2mcmd.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=397484;file:///opt/homebrew/Caskroom/miniconda/base/envs/mixture_of_experts/lib/python3.11/site-packages/git/cmd.py#1253\u001b\\\u001b[2m1253\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m         \u001b[33mcwd\u001b[0m=\u001b[35m/Users/ayushdeolasee/Developer/\u001b[0m\u001b[95mmixture_of_experts\u001b[0m, \u001b[33mstdin\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mvalid\u001b[0m\u001b[39m stream\u001b[0m\u001b[1m>\u001b[0m,        \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m         \u001b[33mshell\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33muniversal_newlines\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                              \u001b[2m           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ayushdeolasee/Developer/mixture_of_experts/wandb/run-20250720_193517-sddvk8u4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pypdeveloper/mixture-of-experts-training/runs/sddvk8u4' target=\"_blank\">moe-64experts-16heads-1024dim</a></strong> to <a href='https://wandb.ai/pypdeveloper/mixture-of-experts-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pypdeveloper/mixture-of-experts-training' target=\"_blank\">https://wandb.ai/pypdeveloper/mixture-of-experts-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pypdeveloper/mixture-of-experts-training/runs/sddvk8u4' target=\"_blank\">https://wandb.ai/pypdeveloper/mixture-of-experts-training/runs/sddvk8u4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">found </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">99</span><span style=\"color: #008000; text-decoration-color: #008000\"> shards for split train</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mfound \u001b[0m\u001b[1;32m99\u001b[0m\u001b[32m shards for split train\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">found </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\"> shards for split val</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mfound \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m shards for split val\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "MAX_SEQ_LENGTH = 1024 \n",
    "WARMUP_STEPS = 750\n",
    "MIN_LR = 3e-6\n",
    "MAX_LR = 3e-4\n",
    "VOCAB_SIZE = 50309\n",
    "NUM_HEADS = 16\n",
    "NUM_EXPERTS = 64\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "EPOCHS = 10000\n",
    "LEARNING_RATE = 3e-4\n",
    "EMBED_DIM = 1024\n",
    "CHECKPOINT_EPOCH = 10\n",
    "train(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_seq_lenght=MAX_SEQ_LENGTH,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_experts=NUM_EXPERTS,\n",
    "    grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    min_lr=MIN_LR,\n",
    "    max_lr=MAX_LR,\n",
    "    max_steps=EPOCHS,\n",
    "    device=device,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    checkpoint_epoch=CHECKPOINT_EPOCH,\n",
    "    project_name=\"mixture-of-experts-training\",\n",
    "    run_name=f\"moe-{NUM_EXPERTS}experts-{NUM_HEADS}heads-{EMBED_DIM}dim\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixture_of_experts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
